# Import necessary modules from LangChain
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationChain
from typing import List, Dict, Any
from umls_rerank import get_umls_keys  # Import function to extract UMLS keys
from model_manager import ModelManager  # Import ModelManager to manage API keys

# Define the MedicalAssistantInterface class to interact with a medical assistant model
class MedicalAssistantInterface:
    def __init__(self, model_manager: ModelManager, model_name: str = "gpt-4-0125-preview"):
        """
        Initializes the medical assistant interface with necessary configurations.
        
        Parameters:
        model_manager (ModelManager): An instance of ModelManager to handle API keys.
        model_name (str): The name of the language model to be used.
        """
        # Retrieve the ChatGPT API key using the ModelManager
        api_key = model_manager.get_api_key("ChatGPT")
        
        # Initialize the ChatOpenAI model with the retrieved API key
        self.llm = ChatOpenAI(model_name=model_name, temperature=0, openai_api_key=api_key)
        
        # Define extended memory for the conversation, setting prefixes and additional context
        self.memory = ExtendedConversationBufferWindowMemory(k=0,
                                                             ai_prefix="Physician",
                                                             human_prefix="Patient",
                                                             extra_variables=["context"])
        
        # Define the template for the conversation prompt and save it as a class attribute
        self.template = """
        Answer the question in conjunction with the following content.
        
        Context:
        {context}
        Current conversation:
        {history}
        Patient: {input}
        Physician:
        """
        
        # Create a PromptTemplate for the conversation using the defined template
        self.PROMPT = PromptTemplate(
            input_variables=["context", "history", "input"], template=self.template
        )
        
        # Set up the conversation chain with the model, memory, and prompt
        self.conversation = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            prompt=self.PROMPT,
            verbose=True,
        )

    # Method to ask a medical question and return a response from the model
    def ask_medical_question(self, question: str) -> str:
        """
        Ask a medical question and return the response based on UMLS.
        
        Parameters:
        question (str): The medical question to be processed.
        
        Returns:
        str: The response generated by the medical assistant.
        """
        # Define the UMLS-specific context prompt
        prompt_context = """
        Question: {question}

        You are interacting with a knowledge graph that contains definitions and relational information of medical terminologies. To provide a precise and relevant answer to this question, you are expected to:

        1. Understand the Question Thoroughly: Analyze the question deeply to identify which specific medical terminologies and their interrelations, as extracted from the knowledge graph, are crucial for formulating an accurate response.

        2. Extract Key Terminologies: Return the 3-5 most relevant medical terminologies based on their significance to the question.

        3. Format the Output : Return in a structured JSON format with the key as "medical terminologies". For example:

        {"medical terminologies": ["term1", "term2", ...]}
        """
        
        # Generate the context based on the question by calling get_umls_keys
        context = get_umls_keys(question, prompt_context, self.llm)
        
        # Use the conversation chain to predict the answer based on the context and question
        answer = self.conversation.predict(context=context, input=question)
        return answer

# Extend the ConversationBufferWindowMemory class to add extra variables to the memory
class ExtendedConversationBufferWindowMemory(ConversationBufferWindowMemory):
    extra_variables: List[str] = []

    @property
    def memory_variables(self) -> List[str]:
        """Always return a list of memory variables, including extra variables."""
        return [self.memory_key] + self.extra_variables

    # Load memory variables including extra variables
    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Return memory with history and extra variables."""
        d = super().load_memory_variables(inputs)
        d.update({k: inputs.get(k) for k in self.extra_variables})
        return d
